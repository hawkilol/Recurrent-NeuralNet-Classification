# -*- coding: utf-8 -*-
"""IA_NN03.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TWz0akYw12_JBoJBsBluRB3r6k2nkJQm
"""

# Commented out IPython magic to ensure Python compatibility.

# TensorFlow and tf.keras
import tensorflow as tf
from tensorflow import keras

#Calculos
import random 
import numpy as np
import os
import sys
import copy

# Plotagem
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import cv2
import IPython
from six.moves import urllib

# instala a versao certa do sg
if 'google.colab' in sys.modules:
#   %pip install -q stellargraph[demos]==1.2.1
  
import stellargraph as sg

try:
    sg.utils.validate_notebook_version("1.2.1")
except AttributeError:
    raise ValueError(
        f"This notebook requires StellarGraph version 1.2.1, but a different version {sg.__version__} is installed.  Please see <https://github.com/stellargraph/stellargraph/issues/1172>."
    ) from None

"""dataset = https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection"""

# carrega o dataframe em csv do git
dataset_url = 'https://gist.githubusercontent.com/hawkilol/816c256c43461f35ace0c4b8c638be6d/raw/d2b21110feaa5785b58cc1d845cdb4236ad75770/YoutubeAlldatasets.csv'
dataframe = pd.read_csv(dataset_url)

dataframe



dataframe.dtypes

dataframe

test_size = 0.3 # tamanho do teste 30%

MAX_NUM_WORDS=280

#texts = dataframe['CONTENT'].tolist()

#seleciona os dados de interesse 
texts = dataframe['CONTENT'].tolist()
labels = dataframe['CLASS'].tolist()

# tamanho das amostras de treino 
train_samples = int(texts.shape[0]*(1 - test_size))

# bota labels para as classes
labels_legend = {'0': 'ham', '1': 'spam'}

labels_legend_inverted = {f"{v}":k for k,v in labels_legend.items()}

#labels_as_int =  [labels_legend[str(x)] for x in labels]



random_idx = random.randint(0, len(texts))

#valida os textos e labels verificando se suas pos existem
assert texts[random_idx] == dataframe.iloc[random_idx].CONTENT
assert labels[random_idx] == dataframe.iloc[random_idx].CLASS
#assert labels_legend_inverted[str(labels_as_int[random_idx])] == labels[random_idx]

#converte o texto dos comentarios em vetores
tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

#assert len(sequences) == len(texts) == len(labels_as_int)

MAX_SEQUENCE_LENGTH = 280

# converte as sequencias de ints de amostras em arrays 2d
x = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)

#mapea os labels para poder ser classificado
y = tf.keras.utils.to_categorical(np.asarray(labels))

#fatia as amostras em treino e teste de acordo com o tamanho das amostras definido anteriormente
train_comments, test_comments, train_labels, test_labels = x[:train_samples],x[train_samples:], y[:train_samples], y[train_samples:]



training_data = {
    'train_comments': train_comments,
    'test_comments': test_comments,
    'train_labels': train_labels,
    'test_labels': test_labels,
    'max_words': MAX_NUM_WORDS,
    'max_sequence': MAX_SEQUENCE_LENGTH,
    'legend': labels_legend,
    'labels_legend_inverted': labels_legend_inverted,
    "tokenizer": tokenizer,
}

print(training_data)

test_comments = training_data['test_comments']
train_comments = training_data['train_comments']
test_labels = training_data['test_labels']
train_labels = training_data['train_labels']

labels_legend_inverted = training_data['labels_legend_inverted']
legend = training_data['legend']
max_sequence = training_data['max_sequence']
max_words = training_data['max_words']
tokenizer = training_data['tokenizer']

#cria o modelo sequencial que permite o empilhamento de camadas
model = keras.Sequential()
MAX_NUM_WORDS = 5000
embed_dim = 128
lstm_out = 196

#adiciona as camadas ao modelo 
model.add(tf.keras.layers.Embedding(MAX_NUM_WORDS, embed_dim, input_length=train_comments.shape[1]))
model.add(tf.keras.layers.SpatialDropout1D(0.4))
model.add(tf.keras.layers.LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.3))
model.add(tf.keras.layers.Dense(2, activation='softmax'))

#compila o modelo de acordo com as metricas escolhidas
model.compile(loss='categorical_crossentropy', optimizer="adam", 
              metrics=[tf.keras.metrics.CategoricalAccuracy(name='cat_accuracy'),
                       tf.keras.metrics.Accuracy(name= 'accuracy'),
                       tf.keras.metrics.Precision(name='precision'),
                       tf.keras.metrics.Recall(name='recall'), 
                       tf.keras.metrics.TrueNegatives(name='true_negatives'),
                       tf.keras.metrics.TruePositives(name='true_positives'),
                       tf.keras.metrics.FalseNegatives(name='false_negatives'),
                       tf.keras.metrics.FalsePositives(name= 'false_positives')])


print(model.summary())

#numero de amostras que vao ser propagadas na nede
batch_size = 32

# numero de viagens das amostras na rede
epochs = 5

#treina o modelo
history = model.fit(train_comments, train_labels, validation_data=(test_comments, test_labels), batch_size=batch_size, verbose=1, epochs=epochs)


sg.utils.plot_history(history)

#calcula a sensibilidade de acordo com o n de true positives e false negatives
def sensitivity(tp, fn):
    sensitivity = tp / (tp + fn)

    return sensitivity

#calcula a sensibilidade de acordo com o n fr true negatives e false positives
def specificity(tn, fp):
    specificity = tn / (tn + fp)
    return specificity

test_loss, test_cat_acc, test_acc, test_precision, test_recall, test_true_negatives, test_true_positives, test_false_negatives, test_false_positives = model.evaluate(test_comments, test_labels)

# funcao para retornar um modelo compilado
def compileModel():
  model = keras.Sequential()
  MAX_NUM_WORDS = 5000
  embed_dim = 128
  lstm_out = 196


  model.add(tf.keras.layers.Embedding(MAX_NUM_WORDS, embed_dim, input_length=train_comments.shape[1]))
  model.add(tf.keras.layers.SpatialDropout1D(0.4))
  model.add(tf.keras.layers.LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.3))
  model.add(tf.keras.layers.Dense(2, activation='softmax'))

  model.compile(loss='categorical_crossentropy', optimizer="adam", 
                metrics=[tf.keras.metrics.CategoricalAccuracy(name='cat_accuracy'),
                        tf.keras.metrics.Accuracy(name= 'accuracy'),
                        tf.keras.metrics.Precision(name='precision'),
                        tf.keras.metrics.Recall(name='recall'), 
                        tf.keras.metrics.TrueNegatives(name='true_negatives'),
                        tf.keras.metrics.TruePositives(name='true_positives'),
                        tf.keras.metrics.FalseNegatives(name='false_negatives'),
                        tf.keras.metrics.FalsePositives(name= 'false_positives')])
  
  print(model.summary())
  
  return model

#funcao para retornar as metricas de avaliacao de um modelo 
def evaluateModel(model):
  
  test_loss, test_cat_acc, test_acc, test_precision, test_recall, test_true_negatives, test_true_positives, test_false_negatives, test_false_positives = model.evaluate(test_comments, test_labels)
  tn = test_true_negatives
  fp = test_false_positives
  tp = test_true_positives
  fn = test_false_negatives

  preds1 = model.predict(test_comments)

  #print('pred', preds1)

  mc = np.array(tf.math.confusion_matrix(test_labels, preds1))
  print('Matriz de confusao: \n', mc)
  print('Test loss(Confiabilidae Negativa):', test_loss)
  print('Test accuracy(Confiabilidade Positiva):', test_acc)
  print('Test categorical accuracy(Confiabilidade Positiva):', test_cat_acc)
  print('Test_precision(Confiabilidade Positiva):', test_precision)

  print('\n')

  print('test_true_negatives:', test_true_negatives)
  print('test_false_positives:', test_false_positives)
  print('test_true_positives:', test_true_positives)
  print('test_false_negatives:', test_false_negatives)

  print('\n')

  print('Especificidade:', specificity(tn, fp))

  print('Sensibilidade:', sensitivity(tp, fn))
  
  return test_loss, test_cat_acc, especificidade, sensibilidade, mc

tn = test_true_negatives
fp = test_false_positives
tp = test_true_positives
fn = test_false_negatives

preds1 = model.predict(test_comments)

#print('pred', preds1)

mc = np.array(tf.math.confusion_matrix(test_labels, preds1))
print('Matriz de confusao: \n', mc)
print('Test loss(Confiabilidae Negativa):', test_loss)
print('Test accuracy(Confiabilidade Positiva):', test_acc)
print('Test categorical accuracy(Confiabilidade Positiva):', test_cat_acc)
print('Test_precision:', test_precision)

print('\n')

print('test_true_negatives:', test_true_negatives)
print('test_false_positives:', test_false_positives)
print('test_true_positives:', test_true_positives)
print('test_false_negatives:', test_false_negatives)

print('\n')
especificidade = specificity(tn, fp)
print('Especificidade:', especificidade)
sensibilidade = sensitivity(tp, fn)
print('Sensibilidade:', sensibilidade)

weights = model.get_weights()

batch_size = 32
epochs = 5
model_list = []
acc_list = []
table = []
data = []


#compila, treina e avalia modelos e os compara
for i in range(10):
  model = copy.deepcopy(compileModel())
  
  history = model.fit(train_comments, train_labels, validation_data=(test_comments, test_labels), batch_size=batch_size, verbose=1, epochs=epochs)

  sg.utils.plot_history(history)

  test_loss, test_cat_acc, especificidade, sensibilidade, mc = evaluateModel(model)
  model_list.append(model)
  acc_list.append(test_cat_acc)
  data = [test_loss, test_cat_acc, especificidade, sensibilidade, mc]
  table.append(data)

print("Media acc:", np.average(acc_list))
print("Desvio acc:", np.std(acc_list))
minAcc = min(acc_list)
print("Min acc:", minAcc)
print("Best model:\n", model_list[acc_list.index(minAcc)].summary())

print("Media acc:", np.average(acc_list))
print("Desvio acc:", np.std(acc_list))
minAcc = min(acc_list)
print("Min acc:", minAcc)
print("Best model:\n", model_list[acc_list.index(minAcc)].summary())

#tabela com as metricas de cada modelo das execucoes
dataf = pd.DataFrame(table, columns = ["Test loss", "Test accuracy", "Especificidade", "Sensibilidade", "Matriz de confusao"])

datafTable = dataf.to_csv('metrics.csv')
dfData = pd.read_csv('metrics.csv')

dfData



# preve a classe de um possivel comentario em String
def predict(text_str, max_words=280, max_sequence = 280, tokenizer=None):
    if not tokenizer:
      return None
    sequences = tokenizer.texts_to_sequences([text_str])
    train_comments = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_sequence)
   
    label_output = model.predict(train_comments)
    
    top_label_index = np.argmax(label_output)
   
    preds = label_output[0]
    pred = preds[top_label_index]
   
    labeled_preds = [{f"{labels_legend[str(i)]}": x} for i, x in enumerate(preds)]
    
    return labeled_preds

# max_sequence = 280
# max_words = 1000

predict("nice video 777777", max_words=max_words, max_sequence=max_sequence, tokenizer=tokenizer)

predict("freebitcoins.com", max_words=max_words, max_sequence=max_sequence, tokenizer=tokenizer)